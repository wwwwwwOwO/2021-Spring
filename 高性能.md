题型： 概念题，简答题，程序设计





# 《CUDA_C_Programming_Guide》

#### 《课件-01-CUDA-C-Basics》

##### Page 2 WHAT IS CUDA? （CUDA概念）

> **CUDA架构**
>
> + Expose GPU parallelism for general-purpose computing 
>
> ​        一种由NVIDIA推出的**通用并行计算架构**，该架构**利用GPU**的并行性解决复杂的计算问题
>
> + Expose/Enable performance
>
>   公开/启用性能
>
> **CUDA C/C++**
>
> + Based	on	industry-standard	C/C++ 
> + Small	set	of	extensions	to	enable	heterogeneous	 programming
> + Straightforward	APIs	to	manage	devices,	memory	etc.

##### Page 8 SIMPLE PROCESSING FLOW  （简单的处理过程）

> ![image-20220112125844805](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112125844805.png)
>
> 1. 将输入数据从 CPU 内存复制到 GPU 内存
> 2. 加载 GPU 程序并执行，在芯片上缓存数据以提高性能
> 3. 将结果从 GPU 内存复制到 CPU 内存

##### Page 10-11 GPU KERNELS: DEVICE CODE

> ![image-20220112130649993](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112130649993.png)
>
> `__global__` 关键字修饰的函数：
>
> + 运行在设备上
> + 由主机调用
>
> nvcc编译器会将源代码部署在主机和设备的器件上
>
> + 设备的函数会被NVIDIA的编译器处理
> + 主机函数会被主机编译器处理
>
> ![image-20220112131126890](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112131126890.png)
>
> 三重尖括号标记对设备代码的调用
>
> + 也称为“kernel launch”
> + 为给定内核调用执行该内核的 CUDA 线程数使用新的 <<<...>>> 执行配置语法指定
> + 执行内核的每个线程都被赋予一个唯一的线程 ID (threadIdx)，可通过内置变量在内核中访问该 ID。
> + 三尖括号内的参数是CUDA内核执行配置

##### Page 12-16 RUNNING CODE IN PARALLEL，VECTOR ADDITION ON THE DEVICE

> ![image-20220112131616326](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112131616326.png)
>
> + 主机和设备内存各自分开的
> + 设备的指针指向 GPU 内存
>   + 通常传递给设备的kernel函数
>   + 通常不在主机代码中解引用
>
> + 主机的指针指向 CPU 内存
>   + 通常不传递给设备代码
>   + 通常不会在设备代码中解引用
>
> + 用于处理设备内存的简单 CUDA API
>   + cudaMalloc()、cudaFree()、cudaMemcpy() 类似于 C 中的 malloc()、free()、memcpy()
>
> ![image-20220112132020171](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112132020171.png)
>
> ![image-20220112132051397](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112132051397.png)
>
> 在以上的例子中，共有N个block，他们的blockIdx不同，每个block中有一个线程，执行一次加法。
>
> ![image-20220112132316258](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112132316258.png)
>
> ![image-20220112132329976](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112132329976.png)
>
> 

#### 《课件-02-CUDA-Shared-Memory》

##### Page 4, 7, 8 SHARING DATA BETWEEN THREADS

> ![image-20220112132513233](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112132513233.png)
>
> ![image-20220112132526350](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112132526350.png)
>
> + 在一个块内，线程通过共享内存共享数据
> + 共享内存相当于一个用户管理的缓存，在程序中显式分配和访问它
> + 特点：访问速度很快，由用户管理
> + 使用`_shared__`关键字，为每个block申请一个共享的变量
>   + 驻留在线程块的共享内存空间中，存在周期和blobk的存在周期相同
>   + block之间是不共享的，每个block中都有一个实例
>   + 只能被block内的所有线程访问
>   + 没有固定的地址
>
> ![image-20220112132539098](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112132539098.png)
>
> 典型的编程模式：
>
> + 将数据从设备内存加载到共享内存
> + 与block中的所有其他线程同步，以便每个线程可以安全地读取由不同线程填充的共享内存位置
> + 处理共享内存中的数据
> + 如有必要，再次进行线程同步以确保共享内存已更新结果
> + 将结果写回设备内存

#### 《课件-03-CUDA-Fundamental-Optimization-Part-1》

##### Page 6 EXECUTION MODEL

> ![image-20220112134658186](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112134658186.png)
>
> + CUDA threads在标量处理器上执行，标量处理器为SIMD处理器的车道
>
> + block被分成warp，在SIMD处理器上执行（Streaming Multiprocessor）
>
> + 多个并发线程块可以驻留在一个 SIMD 处理器上（当线程块的数量大于多处理器时）
>
>   受 SIMD 处理器资源（共享内存和寄存器文件）的限制
>
> + 一个grid发射一个kernel

##### Page 7 WARPS

> ![image-20220112135437737](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112135437737.png)
>
> + 一个block含有多个warps
> + 实际上，一个warp在一个多处理器上执行

##### Page 26 LAUNCH CONFIGURATION: SUMMARY

> ![image-20220112135808615](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112135808615.png)
>
> 为了充分利用GPU，需要足够多数量的线程来使GPU处于busy状态。
>
> + 通常情况下，每个SIMD处理器处理512+CUDA threads
>
> 线程块配置
>
> + 每个block的线程数应为一个warp的线程数的倍数
> + SIMD处理器可以并发的处理至少16个block
>   + block内的线程数太小则不能保持较高的GPU占用率
>   + 太多会使灵活性降低
>   + 通常情况下每个block使用128~256个线程，但最优的方案往往根据程序情况调整 

#### 《课件-04-CUDA-Fundamental-Optimization-Part-2》

##### Page 8-17 GPU MEM OPERATIONS

> ```
> Loads:
> 	Caching
> 		Default mode
> 		Attempts to hit in L1, then L2, then GMEM（全局共享区域）
> 		Load granularity（粒度） is 128-byte line
> 	Non-caching
> 		Compile with –Xptxas –dlcm=cg option to nvcc  
> 		Attempts to hit in L2, then GMEM
> 		Do not hit in L1, invalidate the line if it’s in L1 already
> 		Load granularity is 32-bytes (segment)
> 
> Stores:
> 	Invalidate L1, write-back for L2
> ```
>
> ![image-20220112142327612](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112142327612.png)
>
> Load 操作
>
> + 以warp为单位发起load操作
>
> + 过程
>
>   + 该warp中的CUDA thread提供需要访问的地址
>   + 确定需要哪些行/段
>   + 请求需要的行/段
>
> + caching load（粒度为128字节）
>
>   需要128字节
>
>   + 32字节对齐
>     + 连续：利用率100% `int c = a[idx];`
>     + 不连续：利用率100% `int c = a[rand()%warpSize];`
>   + 非32字节对齐
>     + 连续：利用率50% `int c = a[idx-2];`
>   + 疏散的
>     + 散落在N个缓存行：利用率 128/N\*128（3.125% when N=32） `int c = a[rand()];`
>
>   需要4字节
>
>   + 利用率 4/128=3.125% `int c = a[40];`
>
> + Non-caching load（粒度为32字节）
>
>   需要128字节
>
>   + 疏散的
>     + 散落在N个段：利用率 128/N\*32（12.5% when N=32） `int c = a[rand()];`
>
>  **优化思路**
>
> + Strive for perfect coalescing
>   + (Align starting address - may require padding)（避免非32字节对齐）
>   + A warp should access within a contiguous region（避免疏散的）
>
> + Have enough concurrent accesses to saturate the bus（足够的并发内存访问使总线饱和）
>   + Process several elements per thread（每个线程处理多个元素）
>     + Multiple loads get pipelined（多个负载流水线化）
>     + Indexing calculations can often be reused（索引计算通常可以重复使用）
>   + Launch enough warps to maximize throughput（发射足够多的warp以最大化吞吐量）
>     + Latency is hidden by switching warps
> + Use all the caches!
>

##### Page 20-25 SHARED MEMORY

> + Uses:
>   + Inter-thread communication within a block
>   + Cache data to reduce redundant global memory accesses , like CPU cache
>   + Use it to improve global memory access patterns
>   + The shared memory is expected to be a low-latency memory near each processor core (much like an L1 cache) 
>   + Can be managed (allocate and free) via programming API
>
> + Organization
>   + 32 banks, 4-byte wide banks
>   + Successive 4-byte words belong to different banks
>
> + Performance
>   + Typically: 4 bytes per bank per 1 or 2 clocks per multiprocessor 
>   + shared accesses are issued per 32 threads (warp)
>   + serialization: if *N* threads of 32 access different 4-byte words in the same bank (bank conflict), *N* accesses are executed serially
>
> ![image-20220112152333314](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112152333314.png)
>
> ![image-20220112152348583](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112152348583.png)
>
> ![image-20220112152415153](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112152415153.png)
>
> ![image-20220112152429157](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112152429157.png)

《课件-05_Atomics_Reductions_Warp_Shuffle》

Page12-33 parallel reduction optimization （重要但不在考试范围）

 

《课件-06_Managed_Memory》

Page 5-9 UNIFIED MEMORY

> ![image-20220112152828456](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112152828456.png)
>
> ![image-20220112153016868](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112153016868.png)
>
>  ![image-20220112153156070](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112153156070.png)
>
> ![image-20220112153302569](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112153302569.png)
>
>  ![image-20220112153403707](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112153403707.png)
>
> 总结：相当于CPU和GPU的共享存储器？
>
> 







#  《并行程序设计导论》

## 第一章：

1.1

1.2

1.3

1.4

1.6

## 第二章：

2.1.1

2.1.2

2.2.1 至 2.2.6

2.3.1

2.3.2

2.3.4

2.3.5

2.4.2

2.4.3

2.4.4

2.5

2.6.1至2.6.4

2.7

2.9

## 第三章：

3.1.1至3.1.12

3.2.1

3.2.2

3.3.1

3.3.2

3.4.1至3.4.9

3.5

3.6.1至3.6.4

3.7.1

3.7.2

## 第四章：

4.1

4.2.1至4.2.7

4.3至4.7

4.8.1至4.8.3

4.9.1至4.9.4

4.10

4.11

## 第五章：

5.1至5.5

5.7

5.8

5.9

5.10

## 第六章：

6.1 N-body问题



##  《计算机体系结构-量化研究方法-第六版》

## 第四章（向量处理器、GPU）

## 1. PPT内容

​     Page 5 Flynn’s Taxonomy 弗林分类法

> + *SISD* - Single instruction stream, single data stream
> + *SIMD* - Single instruction stream, multiple data streams
>   + Extend pipelined execution of many data operations, superscalar（扩展许多数据操作的流水线执行，超标量）
>   + Simultaneous parallel data operations in most instruction set. E.g. SSE( streaming SIMD extensions), AVX
>   + New: *SIMT* – Single Instruction Multiple Threads (for GPUs)
> + *MISD* - Multiple instruction streams, single data stream(nNo commercial implementation)
> + *MIMD* - Multiple instruction streams, multiple data streams
>   + Tightly-coupled MIMD (shared memory, NUMAs), OpenMP and Pthreads
>   + Loosely-coupled MIMD (distributed memory system, e.g. cluster), MPI

​     Page 6-7 Advantages of SIMD architectures （SIMD架构的优势）

> 1. Can exploit significant data-level parallelism for:
>    + A set of applications has significant data-level parallelism（具有显着的数据级并行性的程序）
>    + Matrix-oriented scientific computing（面向矩阵的科学计算）
>    + Media-oriented image and sound processors（面向媒体的图像和声音处理器）
>
> 2. More energy efficient than MIMD（相比于MIMD）
>    + Only needs to fetch one instruction per multiple data operations, rather than one instr. per data op.（MIMD需要取出新的指令）
>    + Makes SIMD attractive for personal mobile devices
>
> 3. Allows programmers to continue thinking sequentially

​     Page 11 Example of vector architecture

> ![image-20220112160725671](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112160725671.png)

​     Page 26 Optimizations

> ![image-20220112160823014](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112160823014.png)

​     Page 28 1. A four lane vector unit

> ![image-20220112160838668](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220112160838668.png)
>
> **lane** 包含向量寄存器一部分元素，执行每个功能单元的一个流水线任务（每个向量功能单元使用多条流水线）

​     Page 38-39 Memory banks, 课本 page 298-299

> ![image-20220113152240642](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220113152240642.png)
>
>  为了支持向量处理器的存取，存储系统需要进行相应的设计。
>
> 内存组：为向量载入/存储单元提供带宽
>
> 使用多个内存组的原因
>
> + 控制bank地址独立
> + 存取不连续的字
> + 多个向量处理器共享存储区域
>
> 举例
>
> + 32个处理器，每个需要4 loads、2 stores
> + 周期为2.167ns 你、bank延迟为15ns
> + 若要使所有处理器以全内存带宽运行，最少需要多少个bank？
>   + 32 * 6 = 192，需要192次内存访问
>   + 15ns/2.167ns=7 7个处理器周期
>   + 7*192=1344
>
> ​      Page 40-43![image-20220113155850408](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220113155850408.png)
>
>  ![image-20220113161751332](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220113161751332.png)
>
> 

​     Page 57-58 Roofline Model 课本 Page 307-310

> roofline可视性能模型
>
> ![image-20220113163002809](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220113163002809.png)
>
> 计算密度 = 浮点运算数/所访问的存储器字节数
>
> （计算方法为：获取一个程序的总浮点数运算数，再除以在程序执行期间向主存储器传送的总数据字节）
>
> 峰值浮点性能 可以使用硬件规范求得，由缓存背后的存储器系统确定
>
> 峰值存储器带宽 
>
> 可获得的$GFLOP/s = Min(峰值存储器带宽\times 运算密度, 峰值浮点性能)$
>
> ![image-20220113174238233](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220113174238233.png)
>
> on the sloped portion of the roof the performance is limited by the memory bandwidth, on the flat portion it is limited by peak floating point performance.

​     Page 59-60  C. Graphical Processing Unit GPU

> GPU原用于图形处理，为了使其并行化处理的能力能被更多的应用程序使用：
>
> 基本思路：
>
> + 异构执行模型
>
>   + CPU是主机，GPU是设备
>
> + 为 GPU 开发类似 C 的编程语言
>
>   + 计算统一设备架构 (CUDA)
>
>   + OpenCL 用于独立于供应商的语言
>
> + 将所有形式的 GPU 并行性统一为 CUDA 线程
> + 编程模型：“单指令多线程”（SIMT）
>
> ![image-20220113175639775](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220113175639775.png)

​    Page 62-63 Threads, blocks, and grid (CUDA) 课本 Page 313-320

> ![image-20220113181809556](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220113181809556.png)

​     Page 64-67 Example: multiply two vectors of length 8192

> c
>
> ```c
> // Invoke DAXPY
> daxpy(n, 2.0, x, y);
> // DAXPY in C
> void daxpy(int n = 8192, double a, double *A, double *B, double *C){	
>     for (int i = 0; i < n; ++i)
>     	A[i] = B[i] * C[i];
>  } 
> ```
>
> cuda
>
> ```c
> // Invoke DAXPY with 512 threads per Thread Block
> __host__
> int nblocks = (n + 511) / 512;
> daxpy<<<nblocks, 512 >>>(n, 2.0, A, B, C);
> // DAXPY in CUDA
> __global__
> void daxpy (int n, double a, double *A, double *B, double *C)
> {
>     int i = blockIdx.x*blockDim.x + threadIdx.x;
>     if (i < n) 
> 		A[i] = B[i] * C[i];
> } 
> ```
>
> ![image-20220113182456560](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220113182456560.png)

​     Page 71-72 NVIDIA GPU memory structures 课本 Page 326-328

> ![image-20220113183214170](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220113183214170.png)
>
> ![image-20220113183229329](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220113183229329.png)

​     Page 73 Terminology (GPU)

> ![image-20220113184036318](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220113184036318.png)
>
> ![image-20220113183338593](C:\Users\14637\AppData\Roaming\Typora\typora-user-images\image-20220113183338593.png)

​     Page 68-69 Conditional branching 课本 Page 323 – 326

> 

​     Page 90, 93, 94 Loop level parallelism 

> Assume that a *1-D* array index *i* is **affine**:
>
> + *a* * *i* + *b* (with constants *a* and *b*)
>
> + An *n-D* array index is *affine* if it is affine in each dimension
>
> Assume:
>
> + Store to *a* * *i* + *b*, then
>   + Load from *c* * *i* + *d*
>   + *i* runs from *m* to *n*
>
> + Dependence exists if:
>   + Given *j*, *k* such that *m* ≤ *j* ≤ *n*, *m* ≤ *k* ≤ *n*
>   + Store to *a* x *j* + *b*, load from *c* x *k* + *d*, and *a* * *j* + *b* = *c* * *k* + *d*
>
> Generally cannot determine at compile time
>
> + Test for absence of a dependence:
>
> + GCD test:
>   + If a dependency exists, GCD(*c*,*a*) must evenly divide (*d*-*b*)

2019： 

概念

SIMD和MIMD的区别和联系

共享内存的两个优点和两个缺点

实现同步的两种方式

gpu Shared Memory和Global Memory的区别和联系

简答

解释链表访问冲突

画出Roofline模型、计算、计算矩阵乘法的计算密度，判断性能是由于CPU计算还是访存受限

omp parallel for循环依赖、修改程序、static调度chunksize为2时每个线程负责的元素

编程

MPI梯形法算积分



 

 
